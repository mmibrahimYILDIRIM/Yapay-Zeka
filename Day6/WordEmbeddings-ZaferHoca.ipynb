{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325b076c-e707-4f6a-b692-8e876d616328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a5c2dc-431f-4cab-bc70-4691d59db04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\n",
    "    'color':['red','green','blue','red','green'],\n",
    "    'size':['small','medium','large','medium','small'],\n",
    "    'price':[10,15,20,8,12]\n",
    "    \n",
    "        \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f928c615-638b-404d-9fc6-bcc85945df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>small</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>medium</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>large</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>red</td>\n",
       "      <td>medium</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green</td>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color    size  price\n",
       "0    red   small     10\n",
       "1  green  medium     15\n",
       "2   blue   large     20\n",
       "3    red  medium      8\n",
       "4  green   small     12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251b484e-de2c-44de-b1af-82594c93c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f1640c-6315-4f66-808f-dd065ddaab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size2']=le.fit_transform(df['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b98081-efe6-49ac-b7d4-7c21d14ca028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>size2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>small</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>medium</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>large</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>red</td>\n",
       "      <td>medium</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green</td>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color    size  price  size2\n",
       "0    red   small     10      2\n",
       "1  green  medium     15      1\n",
       "2   blue   large     20      0\n",
       "3    red  medium      8      1\n",
       "4  green   small     12      2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa1d01c-86e4-470e-b78c-eadb26f47647",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8800bc41-27b2-461c-8623-cec4e60c3a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(df[['color']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72da4f32-8479-44e3-8e9e-0c4c7cae3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4958da60-8557-4992-a685-47322404af45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size2</th>\n",
       "      <th>color_green</th>\n",
       "      <th>color_red</th>\n",
       "      <th>size_medium</th>\n",
       "      <th>size_small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  size2  color_green  color_red  size_medium  size_small\n",
       "0     10      2        False       True        False        True\n",
       "1     15      1         True      False         True       False\n",
       "2     20      0        False      False        False       False\n",
       "3      8      1        False       True         True       False\n",
       "4     12      2         True      False        False        True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4533346c-618d-4ad6-b5b4-8a55cd389839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "114db1aa-a4b5-426a-bbb5-a8bd7fa050f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f400c405-bf5f-49a0-8e3f-f90134a1f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aritmetik islem + - / *\n",
    "#King - Man + women= Queen BERT, GENSIM, SPacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81539fb1-1669-4173-957a-0d8e9af57ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumleler= [['Hello','This','is','python','training','by','Aman'],\n",
    "             ['Hello','This','is','Java','training','by','Aman'],\n",
    "             ['Hello','This','is','Data Science','training','by','Unfold','Data','Science'],\n",
    "             ['Hello','This','is','programming','training','']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85e62aeb-b4dd-45dc-80c8-6bc95b03d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04392872-a48c-47d2-a5a2-a1ba3a135cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec(cumleler, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88ac1e70-ba44-430b-a849-1cffe7c3ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e640fe-94f4-45f9-9b8b-52e763298ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d2452-6607-455e-a71c-9f8e5f9d219e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv['Hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d647e6-121e-489f-8fbe-5fd2d2ca922c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('Science', topn=5) #cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420b05e-018e-49be-a830-1f30e29ba712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv['Science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b10a3a78-3c60-4865-a157-ea048eb5d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.078*\"is\" + 0.076*\"in\" + 0.076*\"country.\" + 0.076*\"popular\" + 0.076*\"certain\" + 0.076*\"competition\" + 0.076*\"a\" + 0.076*\"football\" + 0.026*\"tennis\" + 0.026*\"play.\"')\n",
      "(1, '0.055*\"for\" + 0.055*\"training\" + 0.055*\"athletes\" + 0.055*\"olympic.\" + 0.055*\"are\" + 0.055*\"currently\" + 0.055*\"the\" + 0.055*\"there\" + 0.055*\"many\" + 0.054*\"to\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Tennis is my favorite sport to play.\",\n",
    "    \"Football is a popular competition in certain country.\",\n",
    "    \"There are many athletes currently training for the olympic.\"\n",
    "]\n",
    "\n",
    "# Preprocess documents\n",
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "#The LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7d42c29-df35-4287-a5d2-fc15564fa0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/\n",
      "\n",
      "Topics discovered in comedians' transcripts:\n",
      "Topic #1: 0.033*\"you\" + 0.032*\"the\" + 0.027*\"and\" + 0.021*\"that\" + 0.021*\"it\" + 0.019*\"to\" + 0.015*\"in\" + 0.013*\"of\" + 0.012*\"like\" + 0.011*\"he\"\n",
      "\n",
      "Topic #2: 0.000*\"you\" + 0.000*\"the\" + 0.000*\"and\" + 0.000*\"it\" + 0.000*\"that\" + 0.000*\"to\" + 0.000*\"in\" + 0.000*\"of\" + 0.000*\"like\" + 0.000*\"they\"\n",
      "\n",
      "Topic #3: 0.025*\"you\" + 0.022*\"the\" + 0.019*\"to\" + 0.017*\"and\" + 0.017*\"it\" + 0.017*\"my\" + 0.015*\"of\" + 0.012*\"me\" + 0.012*\"that\" + 0.010*\"in\"\n",
      "\n",
      "Topic #4: 0.036*\"and\" + 0.031*\"the\" + 0.028*\"you\" + 0.023*\"to\" + 0.019*\"that\" + 0.018*\"it\" + 0.016*\"of\" + 0.016*\"like\" + 0.012*\"in\" + 0.011*\"was\"\n",
      "\n",
      "Topic #5: 0.033*\"the\" + 0.031*\"you\" + 0.029*\"and\" + 0.021*\"to\" + 0.020*\"that\" + 0.018*\"it\" + 0.017*\"like\" + 0.014*\"of\" + 0.014*\"in\" + 0.012*\"was\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Function to scrape transcript data\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript text specifically from scrapsfromtheloft.com.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(class_=\"ast-container\").find_all('p')]\n",
    "    print(f'Successfully scraped: {url}')\n",
    "    return ' '.join(text)\n",
    "\n",
    "# URLs of transcripts\n",
    "urls = [\n",
    "    'http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/'\n",
    "]\n",
    "\n",
    "# Comedian names (for reference)\n",
    "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']\n",
    "\n",
    "# Scrape transcripts\n",
    "documents = [url_to_transcript(url) for url in urls]\n",
    "\n",
    "# Preprocess the transcripts for LDA modeling\n",
    "texts = [\n",
    "    gensim.utils.simple_preprocess(doc, deacc=True)  # removes punctuations and accents\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA Model setup\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "print(\"\\nTopics discovered in comedians' transcripts:\")\n",
    "for idx, topic in topics:\n",
    "    print(f\"Topic #{idx + 1}: {topic}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "875f3212-0dbb-44a4-ba36-1c397bae37e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Zafer\n",
      "[nltk_data]     Acar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Zafer Acar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Zafer\n",
      "[nltk_data]     Acar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/\n",
      "\n",
      "üìù Topic 0:\n",
      "0.081*\"voice\" + 0.019*\"bo\" + 0.019*\"bro\" + 0.018*\"robotic\" + 0.015*\"repeat\" + 0.011*\"contact\" + 0.010*\"laughter\" + 0.010*\"slut\" + 0.009*\"music\" + 0.009*\"song\"\n",
      "\n",
      "üìù Topic 1:\n",
      "0.021*\"jenny\" + 0.010*\"andy\" + 0.009*\"accident\" + 0.008*\"marriage\" + 0.008*\"scrambler\" + 0.007*\"argument\" + 0.007*\"club\" + 0.006*\"parent\" + 0.006*\"morning\" + 0.006*\"idea\"\n",
      "\n",
      "üìù Topic 2:\n",
      "0.016*\"fuck\" + 0.011*\"applause\" + 0.008*\"facebook\" + 0.007*\"son\" + 0.006*\"afraid\" + 0.006*\"let\" + 0.006*\"brother\" + 0.006*\"toy\" + 0.006*\"choice\" + 0.006*\"laughter\"\n",
      "\n",
      "üìù Topic 3:\n",
      "0.000*\"laughter\" + 0.000*\"hasan\" + 0.000*\"fuck\" + 0.000*\"br\" + 0.000*\"parent\" + 0.000*\"applause\" + 0.000*\"pizza\" + 0.000*\"wn\" + 0.000*\"door\" + 0.000*\"birthday\"\n",
      "\n",
      "üìù Topic 4:\n",
      "0.000*\"voice\" + 0.000*\"fuck\" + 0.000*\"ah\" + 0.000*\"dead\" + 0.000*\"dog\" + 0.000*\"parent\" + 0.000*\"bro\" + 0.000*\"audience\" + 0.000*\"nut\" + 0.000*\"wife\"\n",
      "\n",
      "üìù Topic 5:\n",
      "0.000*\"voice\" + 0.000*\"fuck\" + 0.000*\"parent\" + 0.000*\"dog\" + 0.000*\"audience\" + 0.000*\"ah\" + 0.000*\"let\" + 0.000*\"tit\" + 0.000*\"laughter\" + 0.000*\"bro\"\n",
      "\n",
      "üìù Topic 6:\n",
      "0.010*\"fuck\" + 0.010*\"cunt\" + 0.008*\"audience\" + 0.008*\"girlfriend\" + 0.007*\"nut\" + 0.007*\"dog\" + 0.005*\"american\" + 0.005*\"class\" + 0.005*\"party\" + 0.005*\"chimp\"\n",
      "\n",
      "üìù Topic 7:\n",
      "0.024*\"tit\" + 0.010*\"ok\" + 0.009*\"date\" + 0.009*\"dog\" + 0.009*\"ha\" + 0.009*\"murder\" + 0.008*\"food\" + 0.008*\"older\" + 0.008*\"worst\" + 0.006*\"young\"\n",
      "\n",
      "üìù Topic 8:\n",
      "0.011*\"anthony\" + 0.009*\"husband\" + 0.008*\"shark\" + 0.008*\"grandma\" + 0.008*\"ok\" + 0.007*\"asian\" + 0.006*\"mad\" + 0.006*\"pregnant\" + 0.006*\"na\" + 0.006*\"half\"\n",
      "\n",
      "üìù Topic 9:\n",
      "0.025*\"ah\" + 0.010*\"clinton\" + 0.009*\"wife\" + 0.008*\"fuck\" + 0.007*\"gay\" + 0.007*\"cow\" + 0.005*\"motherfucker\" + 0.005*\"let\" + 0.004*\"american\" + 0.004*\"son\"\n",
      "\n",
      "üìù Topic 10:\n",
      "0.014*\"ck\" + 0.007*\"fuck\" + 0.007*\"religion\" + 0.006*\"stupid\" + 0.005*\"sense\" + 0.005*\"anybody\" + 0.005*\"jesus\" + 0.004*\"wife\" + 0.004*\"cking\" + 0.004*\"dead\"\n",
      "\n",
      "üìù Topic 11:\n",
      "0.018*\"hasan\" + 0.017*\"laughter\" + 0.014*\"br\" + 0.012*\"parent\" + 0.009*\"wn\" + 0.008*\"birthday\" + 0.008*\"dream\" + 0.007*\"bike\" + 0.007*\"bethany\" + 0.007*\"immigrant\"\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Scrape function\n",
    "def url_to_transcript(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    paragraphs = soup.find(class_=\"ast-container\").find_all('p')\n",
    "    text_data = ' '.join(p.text for p in paragraphs)\n",
    "    print(f'Successfully scraped: {url}')\n",
    "    return text_data\n",
    "\n",
    "# Custom stopwords\n",
    "add_stop_words = [\n",
    "    'like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people', 'youre', 'got', \n",
    "    'gonna', 'time', 'think', 'yeah', 'said', 'want', 'say', 'thing', 'going', 'shit',\n",
    "    'fucking', 'didnt', 'hes', 'shes', 'theyre', 'weve', 'ill', 'youve', 'ive', 'couldnt',\n",
    "    'wouldnt', 'cant', 'wont', 'one', 'get', 'really', 'see', '‚ô™', 'g', 'okay'\n",
    "]\n",
    "\n",
    "# Convert frozenset to list for CountVectorizer\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to extract only nouns and adjectives\n",
    "def nouns_adj(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"‚Äô\", \"'\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokenized = word_tokenize(text)\n",
    "    tagged = pos_tag(tokenized)\n",
    "    is_noun_adj = lambda pos: pos.startswith('NN') or pos.startswith('JJ')\n",
    "    words = [lemmatizer.lemmatize(word) for word, pos in tagged if is_noun_adj(pos) and word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# URLs and comedian names\n",
    "urls = [\n",
    "    'http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/'\n",
    "]\n",
    "\n",
    "comedians = ['Louis CK', 'Dave Chappelle', 'Ricky Gervais', 'Bo Burnham', 'Bill Burr', \n",
    "             'Jim Jefferies', 'John Mulaney', 'Hasan Minhaj', 'Ali Wong', \n",
    "             'Anthony Jeselnik', 'Mike Birbiglia', 'Joe Rogan']\n",
    "\n",
    "# Scrape and clean\n",
    "documents = [url_to_transcript(url) for url in urls]\n",
    "cleaned_docs = [nouns_adj(doc) for doc in documents]\n",
    "\n",
    "# Build CountVectorizer\n",
    "cv = CountVectorizer(stop_words=stop_words, max_df=0.8)\n",
    "data_cv = cv.fit_transform(cleaned_docs)\n",
    "\n",
    "# Convert into pandas DataFrame for gensim\n",
    "dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# Create corpus for LDA\n",
    "corpus = gensim.matutils.Sparse2Corpus(data_cv, documents_columns=False)\n",
    "id2word = dict((id, word) for word, id in cv.vocabulary_.items())\n",
    "\n",
    "# LDA Model (12 topics for 12 comedians)\n",
    "lda_model = LdaMulticore(corpus, num_topics=12, id2word=id2word, passes=50, workers=2, random_state=42)\n",
    "\n",
    "# Show topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for idx, topic in topics:\n",
    "    print(f\"\\nüìù Topic {idx}:\\n{topic}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd7c53-3574-417d-8f70-c65001363deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
