{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e4d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33446cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\n",
    "    'color':['red','green','blue','red','green'],\n",
    "    'size':['small','medium','large','medium','small'],\n",
    "    'price':[10,15,20,8,12]\n",
    "    \n",
    "        \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd62bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>small</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>medium</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>large</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>red</td>\n",
       "      <td>medium</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green</td>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color    size  price\n",
       "0    red   small     10\n",
       "1  green  medium     15\n",
       "2   blue   large     20\n",
       "3    red  medium      8\n",
       "4  green   small     12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0915b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renkler birbirine eşit dolayısıyla bir rengi diğerine üstün yapamayız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48168c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a64428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size2']=le.fit_transform(df['size'])    # Bu small med large kısmını sıralıyor büyüklüğe göre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1712e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>size2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>small</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>medium</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>large</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>red</td>\n",
       "      <td>medium</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green</td>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color    size  price  size2\n",
       "0    red   small     10      2\n",
       "1  green  medium     15      1\n",
       "2   blue   large     20      0\n",
       "3    red  medium      8      1\n",
       "4  green   small     12      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ac0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fakat renkler birbirine eşit olduğu için bunların birbirine üstünlüğü olmamalı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502dfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1f0a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(df[['color']]).toarray()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8dba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df,drop_first=True)    # Yada bu şekilde yapıyorz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "720fc471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe ve get_dummies aynı işi yapıyor fakat pipeline da get_dummies kullanmıyoruz get_dummiesı sadece model eğitirken kullanabiiyorz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4f557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aritmetik işlem + - / * kelimeler arasında bu işlemleri yapabiliyoruz mesela 1 resimden diğer resmi çıkarabilyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddcca889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# King-Man+Kadin=Queen  BERET,GENSIM,SPacy paketleri ile bu işlemi yapabiliyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fbcab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d95acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.7-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Using cached thinc-8.3.6-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp39-cp39-win_amd64.whl (633 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.10-cp39-cp39-win_amd64.whl (118 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.13-cp39-cp39-win_amd64.whl (24 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Using cached pydantic_core-2.33.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Using cached blis-1.3.0.tar.gz (2.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting numpy>=1.19.0\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Building wheels for collected packages: blis\n",
      "  Building wheel for blis (pyproject.toml): started\n",
      "  Building wheel for blis (pyproject.toml): finished with status 'error'\n",
      "Failed to build blis\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for blis (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [32 lines of output]\n",
      "  BLIS_COMPILER? None\n",
      "  C:\\Users\\ibrahim\\AppData\\Local\\Temp\\pip-build-env-8knm_1hd\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: BSD License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\about.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\benchmark.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  creating build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\common.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\cy.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\py.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\cy.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\__init__.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  running build_ext\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for blis\n",
      "ERROR: Could not build wheels for blis, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fa899a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumleler= [['Hello','This','is','python','training','by','Aman'],\n",
    "             ['Hello','This','is','Java','training','by','Aman'],\n",
    "             ['Hello','This','is','Data Science','training','by','Unfold','Data','Science'],\n",
    "             ['Hello','This','is','programming','training','']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d68679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec   #Kelimeleri vektöüre çevirioy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69143f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec(cumleler,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca4c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb3b7261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training',\n",
       " 'is',\n",
       " 'This',\n",
       " 'Hello',\n",
       " 'by',\n",
       " 'Aman',\n",
       " '',\n",
       " 'programming',\n",
       " 'Science',\n",
       " 'Data',\n",
       " 'Unfold',\n",
       " 'Data Science',\n",
       " 'Java',\n",
       " 'python']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eb84162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.2426779e-03,  9.2993546e-03, -1.9766092e-04, -1.9672764e-03,\n",
       "        4.6036304e-03, -4.0953159e-03,  2.7431143e-03,  6.9399667e-03,\n",
       "        6.0654259e-03, -7.5107943e-03,  9.3823504e-03,  4.6718083e-03,\n",
       "        3.9661205e-03, -6.2435055e-03,  8.4599797e-03, -2.1501649e-03,\n",
       "        8.8251876e-03, -5.3620026e-03, -8.1294188e-03,  6.8245591e-03,\n",
       "        1.6711927e-03, -2.1985089e-03,  9.5136007e-03,  9.4938548e-03,\n",
       "       -9.7740470e-03,  2.5052286e-03,  6.1566923e-03,  3.8724565e-03,\n",
       "        2.0227872e-03,  4.3050171e-04,  6.7363144e-04, -3.8206363e-03,\n",
       "       -7.1402504e-03, -2.0888723e-03,  3.9238976e-03,  8.8186832e-03,\n",
       "        9.2591504e-03, -5.9759365e-03, -9.4026709e-03,  9.7643770e-03,\n",
       "        3.4297847e-03,  5.1661171e-03,  6.2823449e-03, -2.8042626e-03,\n",
       "        7.3227035e-03,  2.8302716e-03,  2.8710044e-03, -2.3803699e-03,\n",
       "       -3.1282497e-03, -2.3701417e-03,  4.2764368e-03,  7.6057913e-05,\n",
       "       -9.5842788e-03, -9.6655441e-03, -6.1481940e-03, -1.2856961e-04,\n",
       "        1.9974159e-03,  9.4319675e-03,  5.5843508e-03, -4.2906962e-03,\n",
       "        2.7831673e-04,  4.9643586e-03,  7.6983096e-03, -1.1442233e-03,\n",
       "        4.3234206e-03, -5.8143795e-03, -8.0419064e-04,  8.1000505e-03,\n",
       "       -2.3600650e-03, -9.6634552e-03,  5.7792603e-03, -3.9298222e-03,\n",
       "       -1.2228728e-03,  9.9805174e-03, -2.2563506e-03, -4.7570644e-03,\n",
       "       -5.3293873e-03,  6.9808899e-03, -5.7088719e-03,  2.1136629e-03,\n",
       "       -5.2556600e-03,  6.1207139e-03,  4.3573068e-03,  2.6063549e-03,\n",
       "       -1.4910829e-03, -2.7460635e-03,  8.9929365e-03,  5.2157748e-03,\n",
       "       -2.1625196e-03, -9.4703101e-03, -7.4260519e-03, -1.0637414e-03,\n",
       "       -7.9494715e-04, -2.5629092e-03,  9.6827205e-03, -4.5852066e-04,\n",
       "        5.8737611e-03, -7.4475873e-03, -2.5060738e-03, -5.5498634e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['Hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6866af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 0.19912604987621307),\n",
       " ('Hello', 0.0749872624874115),\n",
       " ('Aman', 0.06060013547539711),\n",
       " ('Data', 0.04467390850186348),\n",
       " ('Data Science', 0.03786377236247063)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('Science',topn=5) # cosine similarty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70251f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.5793400e-03,  8.9441938e-03,  4.1661169e-03,  9.2360210e-03,\n",
       "        6.6447114e-03,  2.9237536e-03,  9.8047629e-03, -4.4237422e-03,\n",
       "       -6.8044071e-03,  4.2264368e-03,  3.7297106e-03, -5.6658918e-03,\n",
       "        9.7046075e-03, -3.5574490e-03,  9.5504755e-03,  8.3553279e-04,\n",
       "       -6.3376673e-03, -1.9763205e-03, -7.3779910e-03, -2.9805710e-03,\n",
       "        1.0418643e-03,  9.4821295e-03,  9.3579479e-03, -6.5978598e-03,\n",
       "        3.4757361e-03,  2.2759039e-03, -2.4904648e-03, -9.2290686e-03,\n",
       "        1.0272454e-03, -8.1652552e-03,  6.3218493e-03, -5.8009070e-03,\n",
       "        5.5358205e-03,  9.8337783e-03, -1.5960600e-04,  4.5292717e-03,\n",
       "       -1.8087812e-03,  7.3615811e-03,  3.9408780e-03, -9.0101659e-03,\n",
       "       -2.3977158e-03,  3.6277873e-03, -1.0049843e-04, -1.2012037e-03,\n",
       "       -1.0559720e-03, -1.6705527e-03,  6.0579745e-04,  4.1647307e-03,\n",
       "       -4.2532152e-03, -3.8333302e-03, -5.1814670e-05,  2.6758830e-04,\n",
       "       -1.6902591e-04, -4.7849915e-03,  4.3129898e-03, -2.1712943e-03,\n",
       "        2.1046572e-03,  6.6728680e-04,  5.9697074e-03, -6.8420921e-03,\n",
       "       -6.8172733e-03, -4.4762734e-03,  9.4360569e-03, -1.5926319e-03,\n",
       "       -9.4298124e-03, -5.4377859e-04, -4.4482299e-03,  5.9996773e-03,\n",
       "       -9.5837703e-03,  2.8598884e-03, -9.2539359e-03,  1.2495271e-03,\n",
       "        6.0001621e-03,  7.3991115e-03, -7.6215873e-03, -6.0540237e-03,\n",
       "       -6.8396707e-03, -7.9187099e-03, -9.4991811e-03, -2.1259021e-03,\n",
       "       -8.3693973e-04, -7.2557675e-03,  6.7876317e-03,  1.1190047e-03,\n",
       "        5.8293659e-03,  1.4725361e-03,  7.8940584e-04, -7.3701642e-03,\n",
       "       -2.1761891e-03,  4.3210303e-03, -5.0852061e-03,  1.1305589e-03,\n",
       "        2.8832746e-03, -1.5372114e-03,  9.9326223e-03,  8.3502196e-03,\n",
       "        2.4166387e-03,  7.1178814e-03,  5.8903350e-03, -5.5803736e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['Science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d7f508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.053*\"olympic.\" + 0.053*\"the\" + 0.053*\"currently\" + 0.053*\"for\" + 0.053*\"there\" + 0.053*\"athletes\" + 0.053*\"many\" + 0.053*\"are\" + 0.053*\"training\" + 0.053*\"competition\"')\n",
      "(1, '0.082*\"is\" + 0.081*\"my\" + 0.081*\"play.\" + 0.081*\"to\" + 0.081*\"tennis\" + 0.080*\"favorite\" + 0.080*\"sport\" + 0.027*\"country.\" + 0.027*\"a\" + 0.027*\"certain\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Tennis is my favorite sport to play.\",\n",
    "    \"Football is a popular competition in certain country.\",\n",
    "    \"There are many athletes currently training for the olympic.\"\n",
    "]\n",
    "# Preprocess documents\n",
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#The LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94adf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/\n",
      "\n",
      "Topics discovered in comedians' transcripts:\n",
      "Topic #1: 0.035*\"the\" + 0.034*\"and\" + 0.031*\"you\" + 0.021*\"that\" + 0.021*\"to\" + 0.020*\"it\" + 0.015*\"in\" + 0.015*\"of\" + 0.015*\"like\" + 0.013*\"was\"\n",
      "\n",
      "Topic #2: 0.030*\"the\" + 0.030*\"you\" + 0.025*\"and\" + 0.021*\"that\" + 0.020*\"to\" + 0.019*\"it\" + 0.016*\"of\" + 0.016*\"they\" + 0.015*\"like\" + 0.011*\"in\"\n",
      "\n",
      "Topic #3: 0.019*\"you\" + 0.018*\"my\" + 0.015*\"to\" + 0.013*\"it\" + 0.013*\"the\" + 0.012*\"of\" + 0.012*\"me\" + 0.009*\"and\" + 0.008*\"about\" + 0.008*\"do\"\n",
      "\n",
      "Topic #4: 0.002*\"and\" + 0.001*\"you\" + 0.001*\"the\" + 0.001*\"to\" + 0.001*\"it\" + 0.001*\"that\" + 0.001*\"of\" + 0.001*\"like\" + 0.001*\"my\" + 0.001*\"in\"\n",
      "\n",
      "Topic #5: 0.035*\"you\" + 0.027*\"the\" + 0.022*\"to\" + 0.021*\"and\" + 0.016*\"it\" + 0.015*\"that\" + 0.014*\"like\" + 0.012*\"my\" + 0.011*\"of\" + 0.010*\"is\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Function to scrape transcript data\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript text specifically from scrapsfromtheloft.com.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(class_=\"ast-container\").find_all('p')]\n",
    "    print(f'Successfully scraped: {url}')\n",
    "    return ' '.join(text)\n",
    "\n",
    "# URLs of transcripts\n",
    "urls = [\n",
    "    'http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/'\n",
    "]\n",
    "\n",
    "# Comedian names (for reference)\n",
    "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']\n",
    "\n",
    "# Scrape transcripts\n",
    "documents = [url_to_transcript(url) for url in urls]\n",
    "\n",
    "# Preprocess the transcripts for LDA modeling\n",
    "texts = [\n",
    "    gensim.utils.simple_preprocess(doc, deacc=True)  # removes punctuations and accents\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA Model setup\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "print(\"\\nTopics discovered in comedians' transcripts:\")\n",
    "for idx, topic in topics:\n",
    "    print(f\"Topic #{idx + 1}: {topic}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56ccf078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ibrahim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ibrahim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ibrahim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/\n",
      "Successfully scraped: https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/\n",
      "Successfully scraped: http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/\n",
      "\n",
      "📝 Topic 0:\n",
      "0.000*\"fuck\" + 0.000*\"ah\" + 0.000*\"religion\" + 0.000*\"jesus\" + 0.000*\"fuckin\" + 0.000*\"son\" + 0.000*\"gun\" + 0.000*\"sudden\" + 0.000*\"anybody\" + 0.000*\"parent\"\n",
      "\n",
      "📝 Topic 1:\n",
      "0.014*\"husband\" + 0.012*\"ok\" + 0.010*\"asian\" + 0.009*\"half\" + 0.009*\"pregnant\" + 0.008*\"na\" + 0.007*\"finger\" + 0.007*\"doo\" + 0.006*\"toilet\" + 0.005*\"butt\"\n",
      "\n",
      "📝 Topic 2:\n",
      "0.020*\"anthony\" + 0.015*\"shark\" + 0.014*\"grandma\" + 0.011*\"mad\" + 0.008*\"zealand\" + 0.008*\"san\" + 0.007*\"today\" + 0.007*\"party\" + 0.006*\"gun\" + 0.006*\"francisco\"\n",
      "\n",
      "📝 Topic 3:\n",
      "0.000*\"voice\" + 0.000*\"fuck\" + 0.000*\"parent\" + 0.000*\"audience\" + 0.000*\"ck\" + 0.000*\"clinton\" + 0.000*\"na\" + 0.000*\"president\" + 0.000*\"ah\" + 0.000*\"let\"\n",
      "\n",
      "📝 Topic 4:\n",
      "0.000*\"voice\" + 0.000*\"fuck\" + 0.000*\"husband\" + 0.000*\"ok\" + 0.000*\"half\" + 0.000*\"parent\" + 0.000*\"ck\" + 0.000*\"asian\" + 0.000*\"doo\" + 0.000*\"na\"\n",
      "\n",
      "📝 Topic 5:\n",
      "0.012*\"nut\" + 0.009*\"chimp\" + 0.008*\"jenner\" + 0.008*\"hampstead\" + 0.007*\"rape\" + 0.007*\"twitter\" + 0.007*\"jane\" + 0.007*\"bob\" + 0.006*\"dead\" + 0.006*\"dog\"\n",
      "\n",
      "📝 Topic 6:\n",
      "0.015*\"ck\" + 0.010*\"clinton\" + 0.007*\"president\" + 0.007*\"cow\" + 0.006*\"parent\" + 0.005*\"stupid\" + 0.004*\"office\" + 0.004*\"cking\" + 0.004*\"john\" + 0.004*\"dolphin\"\n",
      "\n",
      "📝 Topic 7:\n",
      "0.046*\"voice\" + 0.011*\"bo\" + 0.010*\"bro\" + 0.010*\"jenny\" + 0.010*\"robotic\" + 0.008*\"repeat\" + 0.007*\"andy\" + 0.006*\"song\" + 0.006*\"contact\" + 0.006*\"slut\"\n",
      "\n",
      "📝 Topic 8:\n",
      "0.000*\"ah\" + 0.000*\"fuck\" + 0.000*\"voice\" + 0.000*\"parent\" + 0.000*\"dog\" + 0.000*\"gun\" + 0.000*\"ok\" + 0.000*\"american\" + 0.000*\"tit\" + 0.000*\"dead\"\n",
      "\n",
      "📝 Topic 9:\n",
      "0.024*\"tit\" + 0.010*\"ok\" + 0.010*\"date\" + 0.009*\"ha\" + 0.009*\"murder\" + 0.009*\"dog\" + 0.008*\"worst\" + 0.008*\"food\" + 0.008*\"older\" + 0.006*\"young\"\n",
      "\n",
      "📝 Topic 10:\n",
      "0.016*\"ah\" + 0.013*\"fuck\" + 0.009*\"gun\" + 0.008*\"american\" + 0.007*\"audience\" + 0.006*\"laughter\" + 0.006*\"hasan\" + 0.006*\"son\" + 0.005*\"girlfriend\" + 0.005*\"cunt\"\n",
      "\n",
      "📝 Topic 11:\n",
      "0.015*\"fuck\" + 0.010*\"gun\" + 0.007*\"helicopter\" + 0.007*\"jesus\" + 0.006*\"sudden\" + 0.006*\"religion\" + 0.006*\"trouble\" + 0.006*\"sense\" + 0.005*\"anybody\" + 0.005*\"hell\"\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Scrape function\n",
    "def url_to_transcript(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    paragraphs = soup.find(class_=\"ast-container\").find_all('p')\n",
    "    text_data = ' '.join(p.text for p in paragraphs)\n",
    "    print(f'Successfully scraped: {url}')\n",
    "    return text_data\n",
    "\n",
    "# Custom stopwords\n",
    "add_stop_words = [\n",
    "    'like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people', 'youre', 'got', \n",
    "    'gonna', 'time', 'think', 'yeah', 'said', 'want', 'say', 'thing', 'going', 'shit',\n",
    "    'fucking', 'didnt', 'hes', 'shes', 'theyre', 'weve', 'ill', 'youve', 'ive', 'couldnt',\n",
    "    'wouldnt', 'cant', 'wont', 'one', 'get', 'really', 'see', '♪', 'g', 'okay'\n",
    "]\n",
    "\n",
    "# Convert frozenset to list for CountVectorizer\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to extract only nouns and adjectives\n",
    "def nouns_adj(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokenized = word_tokenize(text)\n",
    "    tagged = pos_tag(tokenized)\n",
    "    is_noun_adj = lambda pos: pos.startswith('NN') or pos.startswith('JJ')\n",
    "    words = [lemmatizer.lemmatize(word) for word, pos in tagged if is_noun_adj(pos) and word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# URLs and comedian names\n",
    "urls = [\n",
    "    'http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/bo-burnham-what-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "    'https://scrapsfromtheloft.com/comedy/hasan-minhaj-homecoming-king-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "    'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/'\n",
    "]\n",
    "\n",
    "comedians = ['Louis CK', 'Dave Chappelle', 'Ricky Gervais', 'Bo Burnham', 'Bill Burr', \n",
    "             'Jim Jefferies', 'John Mulaney', 'Hasan Minhaj', 'Ali Wong', \n",
    "             'Anthony Jeselnik', 'Mike Birbiglia', 'Joe Rogan']\n",
    "\n",
    "# Scrape and clean\n",
    "documents = [url_to_transcript(url) for url in urls]\n",
    "cleaned_docs = [nouns_adj(doc) for doc in documents]\n",
    "\n",
    "# Build CountVectorizer\n",
    "cv = CountVectorizer(stop_words=stop_words, max_df=0.8)\n",
    "data_cv = cv.fit_transform(cleaned_docs)\n",
    "\n",
    "# Convert into pandas DataFrame for gensim\n",
    "dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# Create corpus for LDA\n",
    "corpus = gensim.matutils.Sparse2Corpus(data_cv, documents_columns=False)\n",
    "id2word = dict((id, word) for word, id in cv.vocabulary_.items())\n",
    "\n",
    "# LDA Model (12 topics for 12 comedians)\n",
    "lda_model = LdaMulticore(corpus, num_topics=12, id2word=id2word, passes=50, workers=2, random_state=42)\n",
    "\n",
    "# Show topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for idx, topic in topics:\n",
    "    print(f\"\\n📝 Topic {idx}:\\n{topic}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "737d479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entriy Recognition NER - Bir yazının içerisinde isimleri, yerleri, konumları ve organizsayonları tanımak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "497971b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_115012\\572880994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52b01f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrahim\\anaconda3\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0a6617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.7-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp39-cp39-win_amd64.whl (633 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Using cached thinc-8.3.6-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.10-cp39-cp39-win_amd64.whl (118 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.13-cp39-cp39-win_amd64.whl (24 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Using cached pydantic_core-2.33.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting numpy>=1.19.0\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Using cached blis-1.3.0.tar.gz (2.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ibrahim\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Building wheels for collected packages: blis\n",
      "  Building wheel for blis (pyproject.toml): started\n",
      "  Building wheel for blis (pyproject.toml): finished with status 'error'\n",
      "Failed to build blis\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for blis (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [32 lines of output]\n",
      "  BLIS_COMPILER? None\n",
      "  C:\\Users\\ibrahim\\AppData\\Local\\Temp\\pip-build-env-36fy5yup\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: BSD License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\about.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\benchmark.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  creating build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\common.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\cy.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\py.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\cy.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\__init__.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  running build_ext\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for blis\n",
      "ERROR: Could not build wheels for blis, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1205c0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_115012\\572880994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(name='en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Apple is looking for a data scientist in Turkey for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d12900",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. \n",
    "He immigrated in 2003 Zafer loves NLP'''\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
